import json
from pathlib import Path
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import os

from src.entity.TechEnglishRAGSystem import TechEnglishRAGSystem
from src.entity.RAGDataProcessor import RAGDataProcessor


def diagnose_rag_issues():
    """
    DiagnÃ³stico completo do sistema RAG
    """
    print("ğŸ” Iniciando diagnÃ³stico do RAG...")

    BASE_PATH = Path(__file__).resolve().parents[2]
    # 1. Verificar se os chunks existem
    chunks_path = os.path.join(BASE_PATH, 'output', "rag_chunks", "processed_chunks.jsonl")
    if not Path(chunks_path).exists():
        print("âŒ Arquivo de chunks nÃ£o encontrado!")
        return False

    # 2. Carregar e analisar os chunks
    chunks = []
    with open(chunks_path, 'r', encoding='utf-8') as f:
        for line in f:
            chunks.append(json.loads(line))

    print(f"ğŸ“Š Total de chunks: {len(chunks)}")

    if len(chunks) == 0:
        print("âŒ Nenhum chunk encontrado no arquivo!")
        return False

    # 3. Analisar o conteÃºdo dos chunks
    total_content_length = 0
    empty_chunks = 0
    sample_contents = []

    for chunk in chunks[:10]:  # Analisar apenas os primeiros 10
        content = chunk.get('content', '')
        total_content_length += len(content)

        if len(content.strip()) < 10:
            empty_chunks += 1
        else:
            sample_contents.append(content[:100])  # Primeiros 100 caracteres

    print(f"ğŸ“ Comprimento mÃ©dio do conteÃºdo: {total_content_length / len(chunks):.0f} caracteres")
    print(f"âš ï¸  Chunks vazios: {empty_chunks}")

    # 4. Mostrar exemplos de conteÃºdo
    print("\nğŸ“ Exemplos de conteÃºdo (primeiros 100 caracteres):")
    for i, content in enumerate(sample_contents[:3]):
        print(f"   {i + 1}. '{content}...'")

    # 5. Analisar metadados
    technologies = {}
    contexts = {}

    for chunk in chunks:
        tech = chunk.get('metadata', {}).get('technology', 'unknown')
        context = chunk.get('metadata', {}).get('professional_context', 'unknown')

        technologies[tech] = technologies.get(tech, 0) + 1
        contexts[context] = contexts.get(context, 0) + 1

    print(f"\nğŸ”§ Tecnologias encontradas: {technologies}")
    print(f"ğŸ¯ Contextos profissionais: {contexts}")

    # 6. Testar o vectorizer com exemplos reais
    print("\nğŸ§ª Testando TF-IDF com consultas reais...")

    # Coletar todo o conteÃºdo para testar o vectorizer
    all_contents = [chunk.get('content', '') for chunk in chunks if chunk.get('content', '').strip()]

    if len(all_contents) > 0:
        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        try:
            tfidf_matrix = vectorizer.fit_transform(all_contents)
            print(f"âœ… Vectorizer treinado com {len(vectorizer.get_feature_names_out())} features")

            # Testar algumas consultas
            test_queries = [
                "bug error problem",
                "function method difference",
                "code review help",
                "api documentation"
            ]

            for query in test_queries:
                query_vec = vectorizer.transform([query])
                similarities = np.dot(query_vec, tfidf_matrix.T).toarray()[0]
                max_similarity = np.max(similarities) if len(similarities) > 0 else 0
                print(f"   '{query}': similaridade mÃ¡xima = {max_similarity:.4f}")

        except Exception as e:
            print(f"âŒ Erro no vectorizer: {e}")
    else:
        print("âŒ Nenhum conteÃºdo vÃ¡lido para treinar o vectorizer")

    return len(chunks) > 0


def fix_rag_system():
    """
    Corrige problemas comuns no sistema RAG
    """
    print("\nğŸ”§ Aplicando correÃ§Ãµes no sistema RAG...")

    processor = RAGDataProcessor(
        chunk_size=600,  # Chunks menores para melhor precisÃ£o
        chunk_overlap=100
    )

    print("ğŸ”„ Reprocessando dados...")
    chunks = processor.process_all_data()

    if len(chunks) == 0:
        print("âŒ Nenhum dado processado. Verificando fontes...")
        check_data_sources()
        return

    # 2. Salvar chunks atualizados
    processor.save_processed_chunks(chunks)

    # 3. Testar com consultas mais especÃ­ficas
    test_with_better_queries()


def check_data_sources():
    """
    Verifica se hÃ¡ dados nas fontes originais
    """
    print("\nğŸ“‚ Verificando fontes de dados...")

    sources = [
        ("technical_docs", "DocumentaÃ§Ãµes tÃ©cnicas"),
        ("github_data", "Dados do GitHub")
    ]

    BASE_PATH = Path(__file__).resolve().parents[2]
    for dir_path, description in sources:
        path = Path(os.path.join(BASE_PATH, 'output',dir_path))
        if path.exists():
            files = list(path.glob("*.json"))
            print(f"âœ… {description}: {len(files)} arquivos")

            # Mostrar alguns arquivos
            for file_path in files[:3]:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        title = data.get('metadata', {}).get('title', 'Sem tÃ­tulo')
                        print(f"   ğŸ“„ {file_path.name}: {title}")
                except:
                    print(f"   âŒ Erro ao ler {file_path.name}")
        else:
            print(f"âŒ {description}: DiretÃ³rio nÃ£o existe")


def test_with_better_queries():
    """
    Testa o RAG com consultas mais especÃ­ficas e tÃ©cnicas
    """
    print("\nğŸ¯ Testando com consultas mais especÃ­ficas...")

    rag_system = TechEnglishRAGSystem()

    # Consultas mais especÃ­ficas baseadas no conteÃºdo tÃ©cnico
    technical_queries = [
        # Baseadas em React
        "React component lifecycle",
        "JSX syntax in React",
        "React hooks useState",
        "React props and state",
        # Baseadas em JavaScript/TypeScript
        "JavaScript function parameters",
        "TypeScript interface definition",
        "async await JavaScript",
        "Node.js module exports",
        # Baseadas em Docker
        "Docker container commands",
        "Dockerfile instructions",
        "docker-compose configuration",
        # Baseadas em Git
        "git commit message",
        "branch merge conflict",
        "GitHub pull request",
        # Gerais de programaÃ§Ã£o
        "database connection string",
        "API endpoint definition",
        "error handling try catch",
        "loop iteration array"
    ]

    print("ğŸ” Executando consultas tÃ©cnicas...")

    successful_queries = 0
    for query in technical_queries:
        results = rag_system.query_rag(query, n_results=3)

        if results['success'] and results['context_chunks']:
            successful_queries += 1
            print(f"âœ… '{query}': {results['total_chunks_found']} resultados")

            # Mostrar o melhor resultado
            best_result = results['context_chunks'][0]
            print(f"   ğŸ† {best_result['technology']} - {best_result['relevance_score']:.3f}")
        else:
            print(f"âŒ '{query}': 0 resultados")

    print(f"\nğŸ“ˆ EstatÃ­sticas: {successful_queries}/{len(technical_queries)} consultas com resultados")


def create_sample_queries_based_on_content():
    """
    Cria consultas baseadas no conteÃºdo real dos chunks
    """
    print("\nğŸ¯ Gerando consultas baseadas no conteÃºdo real...")

    BASE_PATH = Path(__file__).resolve().parents[2]
    chunks_path = os.path.join(BASE_PATH, 'output', "rag_chunks", "processed_chunks.jsonl")
    if not Path(chunks_path).exists():
        print("âŒ Arquivo de chunks nÃ£o encontrado")
        return

    # Coletar palavras-chave dos chunks
    keywords = set()
    sample_sentences = []

    with open(chunks_path, 'r', encoding='utf-8') as f:
        for line in f:
            chunk = json.loads(line)
            content = chunk.get('content', '')

            # Extrair palavras tÃ©cnicas comuns
            technical_terms = [
                'function', 'method', 'class', 'object', 'variable',
                'parameter', 'return', 'import', 'export', 'interface',
                'component', 'state', 'props', 'hook', 'container',
                'image', 'build', 'deploy', 'commit', 'merge', 'branch'
            ]

            for term in technical_terms:
                if term in content.lower():
                    keywords.add(term)

            # Coletar sentenÃ§as completas como exemplos
            sentences = content.split('.')
            for sentence in sentences:
                if len(sentence.strip()) > 20 and len(sentence.strip()) < 100:
                    sample_sentences.append(sentence.strip())
                    if len(sample_sentences) >= 10:
                        break

    print(f"ğŸ”‘ Palavras-chave encontradas: {list(keywords)[:10]}...")
    print(f"ğŸ“ SentenÃ§as de exemplo:")
    for i, sentence in enumerate(sample_sentences[:3]):
        print(f"   {i + 1}. {sentence}")

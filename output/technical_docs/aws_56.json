{
  "metadata": {
    "title": "What is Amazon S3?",
    "url": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html",
    "technology": "aws",
    "category": "devops_cloud",
    "english_level": "B1",
    "professional_context": "deployment",
    "content_type": "technical_documentation",
    "last_updated": "1761765826.5668564"
  },
  "content": "What is Amazon S3?\nAmazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability,\ndata availability, security, and performance. Customers of all sizes and industries can use\nAmazon S3 to store and protect any amount of data for a range of use cases, such as data lakes,\nwebsites, mobile applications, backup and restore, archive, enterprise applications, IoT\ndevices, and big data analytics. Amazon S3 provides management features so that you can optimize,\norganize, and configure access to your data to meet your specific business, organizational,\nand compliance requirements.\nFor more information about using the Amazon S3 Express One Zone storage class with directory buckets, see\nS3 Express One Zone\nWorking with directory buckets\nFeatures of Amazon S3\nHow Amazon S3 works\nAmazon S3 data consistency model\nRelated services\nAccessing Amazon S3\nPaying for Amazon S3\nPCI DSS compliance\nFeatures of Amazon S3\nStorage classes\nAmazon S3 offers a range of storage classes designed for different use cases. For\nexample, you can store mission-critical production data in S3 Standard or S3 Express One Zone for frequent\naccess, save costs by storing infrequently accessed data in S3 Standard-IA or\nS3 One Zone-IA, and archive data at the lowest costs in S3 Glacier Instant Retrieval,\nS3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive.\nAmazon S3 Express One Zone is a high-performance, single-zone Amazon S3 storage class that is purpose-built\nto deliver consistent, single-digit millisecond data access for your most\nlatency-sensitive applications. S3 Express One Zone is the lowest latency cloud object\nstorage class available today, with data access\nup to 10x faster and with request costs\npercent lower than S3 Standard. S3 Express One Zone is the first S3 storage class where you can select a single Availability Zone with\nthe option to co-locate your object storage with your compute resources, which provides the highest possible access speed.\nAdditionally, to further increase access speed and support hundreds of thousands of\nrequests per second, data is stored in a new bucket type: an\nAmazon S3 directory bucket. For more information, see\nS3 Express One Zone\nWorking with directory buckets\nYou can store data with changing or unknown access patterns in\nS3 Intelligent-Tiering, which optimizes storage costs by automatically moving your\ndata between four access tiers when your access patterns change. These four access\ntiers include two low-latency access tiers optimized for frequent and infrequent\naccess, and two opt-in archive access tiers designed for asynchronous access for\nrarely accessed data.\nFor more information, see\nUnderstanding and managing Amazon S3 storage classes\nStorage management\nAmazon S3 has storage management features that you can use to manage costs, meet\nregulatory requirements, reduce latency, and save multiple distinct copies of your\ndata for compliance requirements.\nS3 Lifecycle\n– Configure a lifecycle configuration to manage\nyour objects and store them cost effectively throughout their lifecycle. You\ncan transition objects to other S3 storage classes or expire objects that\nreach the end of their lifetimes.\nS3 Object Lock\n– Prevent Amazon S3 objects from being\ndeleted or overwritten for a fixed amount of time or indefinitely. You can\nuse Object Lock to help meet regulatory requirements that require\nwrite-once-read-many\nstorage or to simply add another\nlayer of protection against object changes and deletions.\nS3 Replication\n– Replicate objects and their respective metadata and object tags to\none or more destination buckets in the same or different AWS Regions for\nreduced latency, compliance, security, and other use cases.\nS3 Batch Operations\n– Manage billions of objects at scale\nwith a single S3 API request or a few clicks in the Amazon S3 console. You can\nuse Batch Operations to perform operations such as\nInvoke AWS Lambda\nmillions or billions of objects.\nAccess management and security\nAmazon S3 provides features for auditing and managing access to your buckets and\nobjects. By default, S3 buckets and the objects in them are private. You have access\nonly to the S3 resources that you create. To grant granular resource permissions\nthat support your specific use case or to audit the permissions of your Amazon S3\nresources, you can use the following features.\nS3 Block Public Access\n– Block public access to S3\nbuckets and objects. By default, Block Public Access settings are turned on\nat the bucket level. We recommend that you keep all Block Public Access\nsettings enabled unless you know that you need to turn off one or more of\nthem for your specific use case. For more information, see\nConfiguring block public access\nsettings for your S3 buckets\nAWS Identity and Access Management (IAM)\n– IAM is a web service that helps\nyou securely control access to AWS resources, including your Amazon S3\nresources. With IAM, you can centrally manage permissions that control\nwhich AWS resources users can access. You use IAM to control who is\nauthenticated (signed in) and authorized (has permissions) to use\n– Use IAM-based policy language to configure\nresource-based permissions for your S3 buckets and the objects in\nAmazon S3 access points\n– Configure named network endpoints with dedicated access policies to\nmanage data access at scale for shared datasets in Amazon S3.\nAccess control\nlists (ACLs)\n– Grant read and write permissions for\nindividual buckets and objects to authorized users. As a general rule, we\nrecommend using S3 resource-based policies (bucket policies and access point\npolicies) or IAM user policies for access control instead of ACLs.\nPolicies are a simplified and more flexible access control option. With\nbucket policies and access point policies, you can define rules that apply\nbroadly across all requests to your Amazon S3 resources. For more information\nabout the specific cases when you'd use ACLs instead of resource-based\npolicies or IAM user policies, see\nManaging access with ACLs\nS3 Object Ownership\n– Take ownership of every object\nin your bucket, simplifying access management for data stored in Amazon S3.\nS3 Object Ownership is an Amazon S3 bucket-level setting that you can use to\ndisable or enable ACLs. By default, ACLs are disabled. With ACLs disabled,\nthe bucket owner owns all the objects in the bucket and manages access to\ndata exclusively by using access-management policies.\nIAM Access Analyzer for S3\n– Evaluate and monitor your S3 bucket access policies, ensuring that\nthe policies provide only the intended access to your S3 resources.\nData processing\nTo transform data and trigger workflows to automate a variety of other processing\nactivities at scale, you can use the following features.\nS3 Object Lambda\n– Add your own code to S3 GET, HEAD, and LIST requests to modify and process data as\nit is returned to an application. Filter rows, dynamically resize images,\nredact confidential data, and much more.\nnotifications\n– Trigger workflows that use Amazon Simple Notification Service\n(Amazon SNS), Amazon Simple Queue Service (Amazon SQS), and AWS Lambda when a change is made to your S3\nStorage logging and monitoring\nAmazon S3 provides logging and monitoring tools that you can use to monitor and control\nhow your Amazon S3 resources are being used. For more information, see\nAutomated monitoring tools\nAmazon CloudWatch\nmetrics for Amazon S3\n– Track the operational health of your\nS3 resources and configure billing alerts when estimated charges reach a\nuser-defined threshold.\nAWS CloudTrail\n– Record actions taken by a user, a role, or an AWS service in\nAmazon S3. CloudTrail logs provide you with detailed API tracking for S3 bucket-level\nand object-level operations.\nManual monitoring tools\nServer access\n– Get detailed records for the requests that are\nmade to a bucket. You can use server access logs for many use cases, such as\nconducting security and access audits, learning about your customer base,\nand understanding your Amazon S3 bill.\nAWS Trusted\n– Evaluate your account by using AWS best\npractice checks to identify ways to optimize your AWS infrastructure,\nimprove security and performance, reduce costs, and monitor service quotas.\nYou can then follow the recommendations to optimize your services and\nAnalytics and insights\nAmazon S3 offers features to help you gain visibility into your storage usage, which\nempowers you to better understand, analyze, and optimize your storage at\nAmazon S3 Storage Lens\n– Understand, analyze, and optimize your storage. S3 Storage Lens provides\n60+ usage and activity metrics and interactive dashboards to aggregate data\nfor your entire organization, specific accounts, AWS Regions, buckets, or\nClass Analysis\n– Analyze storage access patterns to\ndecide when it's time to move data to a more cost-effective storage class.\nS3 Inventory with\nInventory reports\n– Audit and report on objects and their\ncorresponding metadata and configure other Amazon S3 features to take action in\nInventory reports. For example, you can report on the replication and\nencryption status of your objects. For a list of all the metadata available\nfor each object in Inventory reports, see\nAmazon S3 Inventory list\nStrong consistency\nAmazon S3 provides strong read-after-write consistency for PUT and DELETE requests of\nobjects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both\nwrites of new objects as well as PUT requests that overwrite existing objects and\nDELETE requests. In addition, read operations on Amazon S3 Select, Amazon S3 access control\nlists (ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object)\nare strongly consistent. For more information, see\nAmazon S3 data consistency model\nHow Amazon S3 works\nAmazon S3 is an object storage service that stores data as objects, hierarchical data, or tabular data within buckets. An\nis a file and any metadata that describes\nthe file. A\nis a container for objects.\nTo store your data in Amazon S3, you first create a bucket and specify a bucket name and\nAWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object\n), which is the unique identifier for the object within the\nS3 provides features that you can configure to support your specific use case. For\nexample, you can use S3 Versioning to keep multiple versions of an object in the same\nbucket, which allows you to restore objects that are accidentally deleted or\noverwritten.\nBuckets and the objects in them are private and can be accessed only if you explicitly\ngrant access permissions. You can use bucket policies, AWS Identity and Access Management (IAM) policies,\naccess control lists (ACLs), and S3 Access Points to manage access.\nS3 Versioning\nBucket policy\nS3 access points\nAccess control lists (ACLs)\nAmazon S3 supports four types of buckets—general purpose buckets, directory buckets, table buckets, and vector buckets. Each type of bucket provides a unique set of features for different use cases.\nGeneral purpose buckets\n– General purpose buckets are recommended for most use cases and access patterns and are the original S3 bucket type.\nA general purpose bucket is a container for objects stored in Amazon S3, and you can store any number of objects in a bucket and across all storage classes (except for\nS3 Express One Zone), so you can redundantly store objects across multiple Availability Zones. For more information, see\nCreating, configuring, and working with Amazon S3 general purpose buckets\nBy default, all general purpose buckets are private. However, you can grant public access to general purpose buckets.\nYou can control access to general purpose buckets at the bucket, prefix (folder), or object tag level.\nFor more information, see\nAccess control in Amazon S3\nDirectory buckets\n– Recommended for low-latency use cases and data-residency use cases. By default, you can create up to 100 directory buckets in your\nAWS account, with no limit on the number of objects that you can store in a directory bucket. Directory buckets organize objects into hierarchical directories (prefixes) instead of the flat storage structure\nof general purpose buckets. This bucket type has no prefix limits and individual directories can scale horizontally. For more information, see\nWorking with directory buckets\nFor low-latency use cases, you can create a directory bucket in a single AWS Availability Zone to store data. Directory buckets in Availability Zones support the\nS3 Express One Zone storage class. With S3 Express One Zone, your data is redundantly stored on multiple devices within a single Availability Zone. The S3 Express One Zone storage class is recommended if your application\nis performance sensitive and benefits from single-digit millisecond\nlatencies. To learn more about creating directory buckets in Availability Zones, see\nHigh performance workloads\nFor data-residency use cases, you can create a directory bucket in a single AWS Dedicated Local Zone (DLZ) to store data. In Dedicated Local Zones, you can create S3 directory buckets to store data\nin a specific data perimeter, which helps support your data residency and isolation use cases. Directory buckets in Local Zones support the S3 One Zone-Infrequent Access (S3 One Zone-IA; Z-IA) storage class. To learn more about creating\ndirectory buckets in Local Zones, see\nData residency workloads\nDirectory buckets have all public access disabled by default. This behavior can't be changed. You can't grant access to objects stored in directory buckets. You can grant access only to your\ndirectory buckets. For more information, see\nAuthenticating and authorizing requests\nTable buckets\n– Recommended for storing tabular data, such as daily purchase transactions, streaming sensor data, or ad impressions. Tabular data represents data in columns and rows, like in a database table. Table buckets provide\nS3 storage that's optimized for analytics and machine learning workloads, with features designed to continuously improve query performance and reduce storage costs for tables. S3 Tables are purpose-built for\nstoring tabular data in the Apache Iceberg format. You can query tabular data in S3 Tables with popular query engines, including\nAmazon Athena, Amazon Redshift, and Apache Spark. By default, you can create up to 10 table buckets per AWS account per AWS Region\nand up to 10,000 tables per table bucket. For more information, see\nWorking with S3 Tables and table buckets\nAll table buckets and tables are private and can't be made public. These resources can only be accessed by users who are explicitly granted access. To grant access, you can use IAM resource-based policies\nfor table buckets and tables, and IAM identity-based policies for users and roles. For more information, see\nSecurity for S3 Tables\nVector buckets\n– S3 vector buckets are a type of Amazon S3 bucket that are purpose-built to store and query vectors. Vector buckets use dedicated API operations to write and query vector data efficiently. With S3 vector buckets, you can store vector embeddings for machine learning models, perform similarity searches, and integrate with services like Amazon Bedrock and Amazon OpenSearch.\nS3 vector buckets organize data using vector indexes, which are resources within a bucket that store and organize vector data for efficient similarity search. Each vector index can be configured with specific dimensions, distance metrics (like cosine similarity), and metadata configurations to optimize for your specific use case. For more information, see\nWorking with S3 Vectors and vector buckets\nAdditional information about all bucket types\nWhen you create a bucket, you enter a bucket name and choose the AWS Region\nwhere the bucket will reside. After you create a bucket, you cannot change the name\nof the bucket or its Region. Bucket names must follow the following bucket naming rules:\nGeneral purpose bucket naming rules\nDirectory bucket naming rules\nTable bucket naming rules\nBuckets also:\nOrganize the Amazon S3 namespace at the highest level. For general purpose buckets, this namespace is\n. For directory buckets,\nthis namespace is\n. For table buckets, this namespace is\nIdentify the account responsible for storage and data transfer\nServe as the unit of aggregation for usage reporting.\nObjects are the fundamental entities stored in Amazon S3. Objects consist of object\ndata and metadata. The metadata is a set of name-value pairs that describe the\nobject. These pairs include some default metadata, such as the date last modified,\nand standard HTTP metadata, such as\nContent-Type\n. You can also specify\ncustom metadata at the time that the object is stored.\nEvery object is contained in a bucket. For example, if the object named\nphotos/puppy.jpg\nis stored in the\namzn-s3-demo-bucket\ngeneral purpose bucket in the US West (Oregon)\nRegion, then it is addressable by using the URL\nhttps://amzn-s3-demo-bucket.s3.us-west-2.amazonaws.com/photos/puppy.jpg\nFor more information, see\nAccessing a\nAn object is uniquely identified within a bucket by a\nS3 Versioning is enabled on the bucket). For more information about objects, see\nAmazon S3 objects overview\n) is the unique identifier for an object within a bucket. Every\nobject in a bucket has exactly one key. The combination of a bucket, object key, and\noptionally, version ID (if S3 Versioning is enabled for the bucket) uniquely identify\neach object. So you can think of Amazon S3 as a basic data map between \"bucket + key +\nversion\" and the object itself.\nEvery object in Amazon S3 can be uniquely addressed through the combination of the web service\nendpoint, bucket name, key, and optionally, a version. For example, in the URL\namzn-s3-demo-bucket\n.s3.us-west-2.amazonaws.com/photos/puppy.jpg\namzn-s3-demo-bucket\nis the name of the bucket\nphotos/puppy.jpg\nis the key.\nFor more information about object keys, see\nNaming Amazon S3 objects\nS3 Versioning\nYou can use S3 Versioning to keep multiple variants of an object in the same\nbucket. With S3 Versioning, you can preserve, retrieve, and restore every version of\nevery object stored in your buckets. You can easily recover from both unintended\nuser actions and application failures.\nFor more information, see\nRetaining multiple versions of objects with S3 Versioning\nWhen you enable S3 Versioning in a bucket, Amazon S3 generates a unique version ID for\neach object added to the bucket. Objects that already existed in the bucket at the\ntime that you enable versioning have a version ID of\nmodify these (or any other) objects with other operations, such as\n, the new objects\nget a unique version ID.\nFor more information, see\nRetaining multiple versions of objects with S3 Versioning\nBucket policy\nA bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that you can use to\ngrant access permissions to your bucket and the objects in it. Only the bucket owner\ncan associate a policy with a bucket. The permissions attached to the bucket apply\nto all of the objects in the bucket that are owned by the bucket owner. Bucket\npolicies are limited to 20 KB in size.\nBucket policies use JSON-based access policy language that is standard across\nAWS. You can use bucket policies to add or deny permissions for the objects in a\nbucket. Bucket policies allow or deny requests based on the elements in the policy,\nincluding the requester, S3 actions, resources, and aspects or conditions of the\nrequest (for example, the IP address used to make the request). For example, you can\ncreate a bucket policy that grants cross-account permissions to upload objects to an\nS3 bucket while ensuring that the bucket owner has full control of the uploaded\nobjects. For more information, see\nExamples of Amazon S3 bucket policies\nIn your bucket policy, you can use wildcard characters on Amazon Resource Names\n(ARNs) and other values to grant permissions to a subset of objects. For example,\nyou can control access to groups of objects that begin with a common\nor end with a given extension, such as\nS3 access points\nAmazon S3 access points are named network endpoints with dedicated access policies that\ndescribe how data can be accessed using that endpoint. Access points are attached to an\nunderlying data source, such as a general purpose bucket, directory bucket, or a FSx for OpenZFS volume, that you can use to\nperform S3 object operations, such as\n. Access points simplify managing data access at scale for\nshared datasets in Amazon S3.\nEach access point has its own access point policy. You can configure\nBlock Public Access\nfor each access point attached to a bucket. To restrict Amazon S3 data access to a private network, you can\nalso configure any access point to accept requests only from a virtual private cloud\nFor more information about access points for general purpose buckets, see\nManaging access to shared datasets with access points\n. For more information about access points for directory buckets, see\nManaging access to shared datasets in directory buckets with access points\nAccess control lists (ACLs)\nYou can use ACLs to grant read and write permissions to authorized users for\nindividual general purpose buckets and objects. Each general purpose bucket and object has an ACL attached to it as\na subresource. The ACL defines which AWS accounts or groups are granted access and\nthe type of access. ACLs are an access control mechanism that predates IAM. For\nmore information about ACLs, see\nAccess control list (ACL) overview\nS3 Object Ownership is an Amazon S3 bucket-level setting that you can use to both control ownership of the objects that are\nuploaded to your bucket and to disable or enable ACLs. By default, Object Ownership is set to the Bucket owner enforced setting,\nand all ACLs are disabled. When ACLs are disabled, the bucket owner owns all the objects in the bucket and manages access to them\nexclusively by using access-management policies.\nA majority of modern use cases in Amazon S3 no longer require the use of ACLs. We recommend that you keep ACLs disabled, except\nin circumstances where you need to control access for each object individually. With ACLs disabled, you can use policies\nto control access to all objects in your bucket, regardless of who uploaded the objects to your bucket.\nFor more information, see\nControlling ownership of objects and disabling ACLs\nfor your bucket\nYou can choose the geographical AWS Region where Amazon S3 stores the buckets that\nyou create. You might choose a Region to optimize latency, minimize costs, or\naddress regulatory requirements. Objects stored in an AWS Region never leave the\nRegion unless you explicitly transfer or replicate them to another Region.\nFor example, objects stored in the Europe (Ireland) Region\nnever leave it.\nYou can access Amazon S3 and its features only in the AWS Regions that are\nenabled for your account. For more information about enabling a Region to create\nand manage AWS resources, see\nManaging AWS Regions\nAWS General Reference\nFor a list of Amazon S3 Regions and endpoints, see\nRegions and endpoints\nAWS General Reference\nAmazon S3 data consistency model\nAmazon S3 provides strong read-after-write consistency for PUT and DELETE requests of\nobjects in your Amazon S3 bucket in all AWS Regions. This behavior applies to both writes\nto new objects as well as PUT requests that overwrite existing objects and DELETE\nrequests. In addition, read operations on Amazon S3 Select, Amazon S3 access controls lists\n(ACLs), Amazon S3 Object Tags, and object metadata (for example, the HEAD object) are\nstrongly consistent.\nUpdates to a single key are atomic. For example, if you make a PUT request to an\nexisting key from one thread and perform a GET request on the same key from a second\nthread concurrently, you will get either the old data or the new data, but never partial\nor corrupt data.\nAmazon S3 achieves high availability by replicating data across multiple servers within\nAWS data centers. If a PUT request is successful, your data is safely stored. Any read\n(GET or LIST request) that is initiated following the receipt of a successful PUT\nresponse will return the data written by the PUT request. Here are examples of this\nA process writes a new object to Amazon S3 and immediately lists keys within its\nbucket. The new object appears in the list.\nA process replaces an existing object and immediately tries to read it. Amazon S3\nreturns the new data.\nA process deletes an existing object and immediately tries to read it. Amazon S3\ndoes not return any data because the object has been deleted.\nA process deletes an existing object and immediately lists keys within its\nbucket. The object does not appear in the listing.\nAmazon S3 does not support object locking for concurrent writers. If two PUT\nrequests are simultaneously made to the same key, the request with the\nlatest timestamp wins. If this is an issue, you must build an object-locking\nmechanism into your application.\nUpdates are key-based. There is no way to make atomic updates across keys.\nFor example, you cannot make the update of one key dependent on the update\nof another key unless you design this functionality into your\napplication.\nBucket configurations have an eventual consistency model. Specifically, this means\nIf you delete a bucket and immediately list all buckets, the deleted bucket\nmight still appear in the list.\nIf you enable versioning on a bucket for the first time, it might take a short\namount of time for the change to be fully propagated. We recommend that you wait\nfor 15 minutes after enabling versioning before issuing write operations (PUT or\nDELETE requests) on objects in the bucket.\nConcurrent applications\nThis section provides examples of behavior to be expected from Amazon S3 when multiple\nclients are writing to the same items.\nIn this example, both W1 (write 1) and W2 (write 2) finish before the start of R1\n(read 1) and R2 (read 2). Because S3 is strongly consistent, R1 and R2 both return\ncolor = ruby\nIn the next example, W2 does not finish before the start of R1. Therefore, R1\nmight return\ncolor = ruby\ncolor = garnet\nbecause W1 and W2 finish before the start of R2, R2 returns\nIn the last example, W2 begins before W1 has received an acknowledgment.\nTherefore, these writes are considered concurrent. Amazon S3 internally uses\nlast-writer-wins semantics to determine which write takes precedence. However, the\norder in which Amazon S3 receives the requests and the order in which applications\nreceive acknowledgments cannot be predicted because of various factors, such as\nnetwork latency. For example, W2 might be initiated by an Amazon EC2 instance in the same\nRegion, while W1 might be initiated by a host that is farther away. The best way to\ndetermine the final value is to perform a read after both writes have been\nacknowledged.\nRelated services\nAfter you load your data into Amazon S3, you can use it with other AWS services. The\nfollowing are the services that you might use most frequently:\nAmazon Elastic Compute Cloud\n(Amazon EC2)\n– Provides secure and scalable computing\ncapacity in the AWS Cloud. Using Amazon EC2 eliminates your need to invest in\nhardware upfront, so you can develop and deploy applications faster. You can\nuse Amazon EC2 to launch as many or as few virtual servers as you need, configure\nsecurity and networking, and manage storage.\n– Helps businesses, researchers, data\nanalysts, and developers easily and cost-effectively process vast amounts of\ndata. Amazon EMR uses a hosted Hadoop framework running on the web-scale\ninfrastructure of Amazon EC2 and Amazon S3.\n– Helps customers that need to run\noperations in austere, non-data center environments, and in locations where\nthere's a lack of consistent network connectivity. You can use AWS Snow Family\ndevices to locally and cost-effectively access the storage and compute power of\nthe AWS Cloud in places where an internet connection might not be an option.\nAWS Transfer Family\n– Provides fully managed support for\nfile transfers directly into and out of Amazon S3 or Amazon Elastic File System (Amazon EFS) using Secure\nShell (SSH) File Transfer Protocol (SFTP), File Transfer Protocol over SSL\n(FTPS), and File Transfer Protocol (FTP).\nAccessing Amazon S3\nYou can work with Amazon S3 in any of the following ways:\nAWS Management Console\nThe console is a web-based user interface for managing Amazon S3 and AWS resources.\nIf you've signed up for an AWS account, you can access the Amazon S3 console by signing\ninto the AWS Management Console and choosing\nfrom the AWS Management Console home\nAWS Command Line Interface\nYou can use the AWS command line tools to issue commands or build scripts at\nyour system's command line to perform AWS (including S3) tasks.\nAWS Command Line Interface (AWS CLI)\nprovides commands\nfor a broad set of AWS services. The AWS CLI is supported on Windows, macOS, and\nLinux. To get started, see the\nAWS Command Line Interface User Guide\n. For more information about the commands for\nAmazon S3, see\nAWS CLI Command Reference\nAWS provides SDKs (software development kits) that consist of libraries and sample code\nfor various programming languages and platforms (Java, Python, Ruby, .NET, iOS,\nAndroid, and so on). The AWS SDKs provide a convenient way to create programmatic\naccess to S3 and AWS. Amazon S3 is a REST service. You can send requests to Amazon S3 using\nthe AWS SDK libraries, which wrap the underlying Amazon S3 REST API and simplify your\nprogramming tasks. For example, the SDKs take care of tasks such as calculating\nsignatures, cryptographically signing requests, managing errors, and retrying\nrequests automatically. For information about the AWS SDKs, including how to\ndownload and install them, see\nEvery interaction with Amazon S3 is either authenticated or anonymous. If you are using\nthe AWS SDKs, the libraries compute the signature for authentication from the keys\nthat you provide. For more information about how to make requests to Amazon S3, see\nMaking requests\nAmazon S3 REST API\nThe architecture of Amazon S3 is designed to be programming language-neutral, using\nAWS-supported interfaces to store and retrieve objects. You can access S3 and\nAWS programmatically by using the Amazon S3 REST API. The REST API is an HTTP interface\nto Amazon S3. With the REST API, you use standard HTTP requests to create, fetch, and\ndelete buckets and objects.\nTo use the REST API, you can use any toolkit that supports HTTP. You can even use\na browser to fetch objects, as long as they are anonymously readable.\nThe REST API uses standard HTTP headers and status codes, so that standard\nbrowsers and toolkits work as expected. In some areas, we have added functionality\nto HTTP (for example, we added headers to support access control). In these cases,\nwe have done our best to add the new functionality in a way that matches the style\nof standard HTTP usage.\nIf you make direct REST API calls in your application, you must write the code to\ncompute the signature and add it to the request. For more information about how to\nmake requests to Amazon S3, see\nMaking requests\nAmazon S3 API Reference\nSOAP API support over HTTP is deprecated, but it is still available over\nHTTPS. Newer Amazon S3 features are not supported for SOAP. We recommend that you use\neither the REST API or the AWS SDKs.\nPaying for Amazon S3\nPricing for Amazon S3 is designed so that you don't have to plan for the storage\nrequirements of your application. Most storage providers require you to purchase a\npredetermined amount of storage and network transfer capacity. In this scenario, if you\nexceed that capacity, your service is shut off or you are charged high overage fees. If\nyou do not exceed that capacity, you pay as though you used it all.\nAmazon S3 charges you only for what you actually use, with no hidden fees and no overage\ncharges. This model gives you a variable-cost service that can grow with your business\nwhile giving you the cost advantages of the AWS infrastructure. For more information,\nAmazon S3 Pricing\nWhen you sign up for AWS, your AWS account is automatically signed up for all\nservices in AWS, including Amazon S3. However, you are charged only for the services that\nyou use. If you are a new Amazon S3 customer, you can get started with Amazon S3 for free. For\nmore information, see\nAWS free tier\nTo see your bill, go to the Billing and Cost Management Dashboard in the\nAWS Billing and Cost Management console\n. To learn more about AWS account billing, see the\nAWS Billing User Guide\n. If you have\nquestions concerning AWS billing and AWS accounts, contact\nAWS Support\nPCI DSS compliance\nAmazon S3 supports the processing, storage, and transmission\nof credit card data by a merchant or service provider, and has been\nvalidated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS).\nFor more information about PCI DSS, including how to request a copy of the AWS PCI Compliance Package,\nPCI DSS Level 1",
  "word_count": 5365,
  "key_terms": [
    "authentication",
    "return",
    "framework",
    "configuration",
    "class",
    "database",
    "object",
    "api",
    "interface"
  ]
}